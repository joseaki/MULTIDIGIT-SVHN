{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import json, codecs\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.hub import load_state_dict_from_url\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import random\n",
    "import glob\n",
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "from matplotlib.patches import Rectangle\n",
    "from skimage import io, transform\n",
    "\n",
    "from skimage.transform import resize\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxes(path, index, f):\n",
    "    boxes = []\n",
    "    labels = np.asarray([])\n",
    "    bbox_data = f[\"/digitStruct/bbox\"]\n",
    "    n_boxes = f[bbox_data[index][0]][\"label\"].len()\n",
    "    image_path = \"\"\n",
    "    if(n_boxes == 1):\n",
    "        box = {}\n",
    "        box['height'] = f[bbox_data[index][0]][\"height\"][0][0]\n",
    "        box['label'] = f[bbox_data[index][0]][\"label\"][0][0]\n",
    "        labels = np.append(labels,0) if (box['label'] == 10) else np.append(labels,box['label'])\n",
    "        box['left'] = f[bbox_data[index][0]][\"left\"][0][0]\n",
    "        box['top'] = f[bbox_data[index][0]][\"top\"][0][0]\n",
    "        box['width'] = f[bbox_data[index][0]][\"width\"][0][0]\n",
    "        box['name'] = str(index+1) + \".png\"\n",
    "        image_path = str(index+1) + \".png\"\n",
    "        boxes.append(box)\n",
    "    else:\n",
    "        for i in range(n_boxes):\n",
    "            box = {}\n",
    "            box['height'] = f[f[bbox_data[index][0]][\"height\"][i][0]][()][0][0]\n",
    "            box['label'] = f[f[bbox_data[index][0]][\"label\"][i][0]][()][0][0]\n",
    "            labels = np.append(labels,0) if (box['label'] == 10) else np.append(labels,box['label'])\n",
    "            box['left'] = f[f[bbox_data[index][0]][\"left\"][i][0]][()][0][0]\n",
    "            box['top'] = f[f[bbox_data[index][0]][\"top\"][i][0]][()][0][0]\n",
    "            box['width'] = f[f[bbox_data[index][0]][\"width\"][i][0]][()][0][0]\n",
    "            box['name'] = str(index+1) + \".png\"\n",
    "            image_path = str(index+1) + \".png\"\n",
    "            boxes.append(box)\n",
    "    image = io.imread(os.path.join(path,image_path))\n",
    "    bounds = get_borders(boxes)\n",
    "    y1 = int(bounds[1]) if int(bounds[1])>=0 else 0\n",
    "    x1 = int(bounds[3]) if int(bounds[3])>=0 else 0\n",
    "    y2 = int(bounds[0]) if int(bounds[0])>=0 else 0\n",
    "    x2 = int(bounds[2]) if int(bounds[2])>=0 else 0\n",
    "    image = image[y1:x1, y2:x2]\n",
    "    \n",
    "    image = np.multiply(resize(image, (64, 64)),255.0).tolist()\n",
    "    for i in range(5 - len(labels)):\n",
    "        labels = np.insert(labels, 0, 10, 0)\n",
    "    #image = torch.from_numpy(image)\n",
    "    #labels = torch.tensor(labels)\n",
    "    return image,labels,boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_SVHN(folder_path):\n",
    "    json_file = {}\n",
    "    print(\"loading data from \"+folder_path)\n",
    "    nh5 = len(glob.glob1(folder_path,\"*.h5\"))\n",
    "    try:\n",
    "        imgs, lbls  = read_many_hdf5(folder_path, 0)\n",
    "        for i in range(1,nh5):\n",
    "            images, labels  = read_many_hdf5(folder_path, i)\n",
    "            imgs = np.append(imgs,images, axis=0)\n",
    "            lbls = np.append(lbls,labels, axis=0)\n",
    "        data = [imgs,lbls]\n",
    "    except:\n",
    "        print(\"Archivo no existe, generando nuevos datos ...\")\n",
    "        data = read_data(folder_path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    with h5py.File(path+\"/digitStruct.mat\", \"r\") as f:    \n",
    "        n_data = f[\"/digitStruct/bbox\"].len()\n",
    "        img_data = []\n",
    "        lbl_data = []\n",
    "        n=0\n",
    "        print(\"creando archivo de imagenes\")\n",
    "        for i in range(n_data):\n",
    "            image, label, boxes = get_boxes(path, i, f)\n",
    "            if(len(boxes)<=5):\n",
    "                img_data.append(image)\n",
    "                lbl_data.append(label)\n",
    "            if(i%10000==9999):\n",
    "                print(\"guardando archivo de iamgenes....\")\n",
    "                store_many_hdf5(img_data, lbl_data, path, n)\n",
    "                img_data = []\n",
    "                lbl_data = []\n",
    "                n = n + 1\n",
    "        store_many_hdf5(img_data, lbl_data, path, n)\n",
    "        \n",
    "        return img_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_many_hdf5(images, labels, path, name):\n",
    "    \"\"\" Stores an array of images to HDF5.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        images       images array, (N, 32, 32, 3) to be stored\n",
    "        labels       labels array, (N, 1) to be stored\n",
    "    \"\"\"\n",
    "    hdf5_dir = Path(path)\n",
    "    hdf5_dir.mkdir(parents=True, exist_ok=True)\n",
    "    num_images = len(images)\n",
    "\n",
    "    # Create a new HDF5 file\n",
    "    file = h5py.File(hdf5_dir / f\"{name}.h5\", \"w\")\n",
    "\n",
    "    # Create a dataset in the file\n",
    "    dataset = file.create_dataset(\n",
    "        \"images\", np.shape(images), h5py.h5t.STD_U8BE, data=images\n",
    "    )\n",
    "    meta_set = file.create_dataset(\n",
    "        \"meta\", np.shape(labels), h5py.h5t.STD_U8BE, data=labels\n",
    "    )\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_many_hdf5(path, name):\n",
    "    \"\"\" Reads image from HDF5.\n",
    "        Parameters:\n",
    "        ---------------\n",
    "        num_images   number of images to read\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        images      images array, (N, 32, 32, 3) to be stored\n",
    "        labels      associated meta data, int label (N, 1)\n",
    "    \"\"\"\n",
    "    hdf5_dir = Path(path)\n",
    "    hdf5_dir.mkdir(parents=True, exist_ok=True)\n",
    "    images, labels = [], []\n",
    "\n",
    "    # Open the HDF5 file\n",
    "    file = h5py.File(hdf5_dir / f\"{name}.h5\", \"r+\")\n",
    "\n",
    "    images = np.array(file[\"/images\"]).astype(\"uint8\")\n",
    "    labels = np.array(file[\"/meta\"]).astype(\"uint8\")\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_borders(img_data):\n",
    "    x1 = 9999999\n",
    "    y1 = 9999999\n",
    "    x2 = 0\n",
    "    y2 = 0\n",
    "    for box in img_data:\n",
    "        if(box[\"top\"]<y1): y1 = box[\"top\"] \n",
    "        if(box[\"left\"]<x1): x1 = box[\"left\"]\n",
    "        if(box[\"top\"] + box[\"height\"] > y2): y2 = box[\"top\"] + box[\"height\"]\n",
    "        if(box[\"left\"] + box[\"width\"] > x2): x2 = box[\"left\"] + box[\"width\"]\n",
    "\n",
    "    return [x1, y1, x2, y2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class svhn_dataset(Dataset):\n",
    "    def __init__(self, folder_path, transform = None):\n",
    "        self.folder_path = folder_path\n",
    "        self.data = load_SVHN(folder_path=folder_path)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data[1])\n",
    "    def __getitem__(self, idx):\n",
    "        #image = np.moveaxis(self.data[0][idx], -1, 0)\n",
    "        image = self.data[0][idx]\n",
    "        labels = self.data[1][idx]\n",
    "        if(self.transform):\n",
    "            image = self.transform(image)\n",
    "        sample = {\"image\": image, \"labels\": torch.tensor(labels)}\n",
    "        return(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class toPIL(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "        # swap color axis because\n",
    "        # numpy image: C x H x W\n",
    "        # torch image: C X H X W\n",
    "        #image = image.transpose((2, 0, 1))\n",
    "        # print(other[0][\"name\"],labels)\n",
    "        image = Image.fromarray(image, mode=\"RGB\")\n",
    "        \n",
    "        #image = TF.normalize(image,mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = svhn_dataset(\"./data/SVHN/test\",transform=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                             transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = svhn_dataset(\"./data/SVHN/train\",transform=transforms.Compose([toPIL(),\n",
    "                                                                               transforms.RandomChoice([\n",
    "                                                                                   transforms.ColorJitter(brightness=[0.5,0.8],\n",
    "                                                                                                          contrast=[0.4,0.5],\n",
    "                                                                                                          saturation=[0,1]),\n",
    "                                                                                   transforms.RandomRotation(degrees=30)\n",
    "                                                                               ]),\n",
    "                                                                               transforms.ColorJitter(hue=[-0.5,0.5]),\n",
    "                                                                               transforms.ToTensor(),\n",
    "                                                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                                                                              ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_dataset = svhn_dataset(\"./data/SVHN/extra\",transform=transforms.Compose([\n",
    "                                                                               transforms.ToTensor(),\n",
    "                                                                               transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                                                                              ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size = 256, num_workers = 0, shuffle = True, drop_last = True)\n",
    "validation_loader = DataLoader(test_dataset, batch_size = 256, num_workers = 0, shuffle = True, drop_last = True)\n",
    "test_loader = DataLoader(extra_dataset, batch_size = 128, num_workers = 0, shuffle = True, drop_last = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training set has {} batches \".format(len(train_loader)))\n",
    "print(\"validation set has {} batches \".format(len(validation_loader)))\n",
    "print(\"test set has {} batches \".format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = train_dataset[26][\"image\"]\n",
    "print(image)\n",
    "plt.figure()\n",
    "plt.imshow(image.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
    "    'vgg19_bn', 'vgg19',\n",
    "]\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n",
    "    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n",
    "    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n",
    "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n",
    "    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n",
    "    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n",
    "    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n",
    "    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "\n",
    "    def __init__(self, features, num_classes=11, init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((2, 2))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 2 * 2, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "        )\n",
    "        self.fcd1 = nn.Linear(4096, num_classes)\n",
    "        self.fcd2 = nn.Linear(4096, num_classes)\n",
    "        self.fcd3 = nn.Linear(4096, num_classes)\n",
    "        self.fcd4 = nn.Linear(4096, num_classes)\n",
    "        self.fcd5 = nn.Linear(4096, num_classes)\n",
    "        self.sig1 = nn.LogSoftmax(dim=1)\n",
    "        self.sig2 = nn.LogSoftmax(dim=1)\n",
    "        self.sig3 = nn.LogSoftmax(dim=1)\n",
    "        self.sig4 = nn.LogSoftmax(dim=1)\n",
    "        self.sig5 = nn.LogSoftmax(dim=1)\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        dig1 = self.sig1(self.fcd1(x))\n",
    "        dig2 = self.sig1(self.fcd2(x))\n",
    "        dig3 = self.sig1(self.fcd3(x))\n",
    "        dig4 = self.sig1(self.fcd4(x))\n",
    "        dig5 = self.sig1(self.fcd5(x))\n",
    "        resp = torch.stack([dig1, dig2, dig3, dig4, dig5],2) \n",
    "        return resp\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfgs = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "def _vgg(arch, cfg, batch_norm, pretrained, progress, **kwargs):\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg11(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 11-layer model (configuration \"A\") from\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg11', 'A', False, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg11_bn(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg11_bn', 'A', True, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg13(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 13-layer model (configuration \"B\")\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg13', 'B', False, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg13_bn(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg13_bn', 'B', True, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg16(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 16-layer model (configuration \"D\")\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg16', 'D', False, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg16_bn(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg16_bn', 'D', True, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg19(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 19-layer model (configuration \"E\")\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg19', 'E', False, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg19_bn(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg19_bn', 'E', True, pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for idx, data in enumerate(dataloaders[phase]):\n",
    "                inputs = data[\"image\"].to(device, dtype=torch.float)\n",
    "                labels = data[\"labels\"].to(device, dtype=torch.long)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    \n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    _,preds = torch.max(outputs, 1)#outputs.data.max(1, keepdim=False) #outputs.data.max(2, keepdim=False)[1].reshape(-1,5).float()\n",
    "                    \n",
    "                   \n",
    "                    if(idx == 20 and phase == 'val' ):\n",
    "                        #p = pd.DataFrame(outputs[0,:,:].cpu().detach().numpy())\n",
    "                        #display(p)\n",
    "                        print(\"Predicciones\",preds[0,:])\n",
    "                        print(\"Real .......\",labels[0,:])\n",
    "                    #_, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += preds.eq(labels).sum() #torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "scratch_model = vgg11_bn()\n",
    "\n",
    "# Print the model we just instantiated\n",
    "print(scratch_model)\n",
    "\n",
    "params_to_update = scratch_model.parameters()\n",
    "print(\"Params to learn:\")\n",
    "\n",
    "for name,param in scratch_model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        print(\"\\t\",name)\n",
    "\n",
    "dataloaders_dict = {\"train\":train_loader,\"val\":validation_loader}\n",
    "scratch_model = scratch_model.to(device)\n",
    "scratch_optimizer = optim.Adam(scratch_model.parameters(), lr=0.0001)\n",
    "scratch_criterion = nn.CrossEntropyLoss()\n",
    "_,scratch_hist = train_model(scratch_model, dataloaders_dict, scratch_criterion, scratch_optimizer, num_epochs=150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for idx,data in enumerate(train_loader, 0):\n",
    "    torch.save(data,\"./\"+str(idx)+\"data.pt\")\n",
    "for idx,data in enumerate(validation_loader, 0):\n",
    "    torch.save(data,\"./\"+str(idx)+\"data_val.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './svhn_netvgg11_adam.pth'\n",
    "torch.save(scratch_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "PATH = './svhn_netvgg11_adam.pth'\n",
    "model = vgg11_bn()\n",
    "model = model.to(device)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.eval()\n",
    "correctos = 0.0\n",
    "incorrectos = 0\n",
    "total_imagenes = 0\n",
    "for i, data in enumerate(test_loader,0):\n",
    "    image = data[\"image\"].to(device, dtype=torch.float)\n",
    "    label = data[\"labels\"].to(device, dtype=torch.long)\n",
    "    #draw_boxes(data[\"image\"][0], data[\"other\"])\n",
    "    totalbatch = len(image)\n",
    "    total_imagenes += totalbatch\n",
    "    out = model(image)\n",
    "    #print(out)\n",
    "    #print(label)\n",
    "    _,preds = torch.max(out, 1)\n",
    "    #print(predicted)\n",
    "    cor = preds.eq(label).sum()\n",
    "    #print(predicted.eq(label.float()))\n",
    "    \n",
    "    \n",
    "    correctos += cor\n",
    "    incorrectos += totalbatch*5 - (cor)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(correctos, incorrectos, total_imagenes)\n",
    "x = ((correctos.item())/(total_imagenes*5))*100\n",
    "print(\"accuracy {}%\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "shist = [h.cpu().numpy() for h in scratch_hist]\n",
    "plt.plot(range(1,150+1),shist,label=\"Scratch\")\n",
    "plt.xticks(np.arange(0, 151, 15.0))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cv)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
